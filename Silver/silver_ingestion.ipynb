{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bcd29787-2322-461e-9a99-dbe7de247c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dotenv\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72467703-8aea-4764-be28-f3169521f066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Importing helper functins with reload, so that changes in that file are reflected here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452f244b-d956-424a-a6c1-2b268df24db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Only needed if you've already imported before\n",
    "import helper_functions\n",
    "from importlib import reload\n",
    "reload(helper_functions)\n",
    "\n",
    "# Now import the function\n",
    "from helper_functions import sanitize_name, add_ingestion_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de717e9-f38c-4366-a2bb-e24a98b79a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d235701b-be41-4071-840f-dfdbf0c629c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from functools import reduce\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af18d451-6d45-479b-8f6a-37460e474236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ddac32-c778-45d3-80cc-9d7fe8d39612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading variable values from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c35cc1-9440-455c-ba5f-6afcd5ca337c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = os.getenv(\"catalog\")\n",
    "bronze_schema = os.getenv(\"bronze_schema\")\n",
    "silver_schema = os.getenv(\"silver_schema\")\n",
    "container = os.getenv(\"container\")\n",
    "storage_account = os.getenv(\"storage_account\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc49f202-cd89-4737-b963-1a82caf6ec07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e042d30-3829-4851-9c18-7ec328597faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS veersadatabricks.silver\n",
    "LOCATION 'abfss://veersacontainer@storage12092004.dfs.core.windows.net/silver/';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be1ebfb4-7be3-4743-9bfc-50bdf7b56b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5a0aca-fe46-488b-9c68-a7fa6f2a9d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_tables = [t.name for t in spark.catalog.listTables(f\"{catalog}.{bronze_schema}\") if t.tableType == \"MANAGED\" or t.tableType == \"EXTERNAL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ece20aa3-54e0-48fd-b39b-76aac531c190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from delta.tables import DeltaTable\n",
    "# import pyspark.sql.functions as F\n",
    "\n",
    "# def run_scd2_for_patientinfo(catalog, bronze_schema, silver_schema):\n",
    "#     # Read Bronze patientinfo\n",
    "#     bronze_df = spark.read.table(f\"{catalog}.{bronze_schema}.patientinfo\")\n",
    "\n",
    "#     # Basic cleaning (rename cols, lowercase)\n",
    "#     for col in bronze_df.columns:\n",
    "#         new_col = sanitize_name(col)\n",
    "#         if new_col != col:\n",
    "#             bronze_df = bronze_df.withColumnRenamed(col, new_col)\n",
    "#     bronze_df = bronze_df.select([F.col(c).alias(c.lower()) for c in bronze_df.columns])\n",
    "\n",
    "#     # Add start_date (use ingested_at or _source_file_mod_ts from Bronze)\n",
    "#     bronze_df = bronze_df.withColumn(\"effective_start_date\", F.col(\"_ingested_at\")) \\\n",
    "#                         .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\")) \\\n",
    "#                         .withColumn(\"is_current\", F.lit(True))\n",
    "\n",
    "#     silver_table_path = f\"{catalog}.{silver_schema}.patientinfo_silver\"\n",
    "\n",
    "#     # If table exists, perform SCD2 merge\n",
    "#     if DeltaTable.isDeltaTable(spark, f\"/user/hive/warehouse/{silver_schema}.db/patientinfo_silver\"):\n",
    "#         silver_tbl = DeltaTable.forName(spark, silver_table_path)\n",
    "\n",
    "#         # Join condition on natural key\n",
    "#         join_cond = \"tgt.patient_id = src.patient_id\"\n",
    "\n",
    "#         # Columns to compare for changes (exclude metadata columns)\n",
    "#         compare_cols = [\n",
    "#         \"sex\", \"age\", \"country\", \"province\", \"city\", \"infection_case\",\n",
    "#         \"infected_by\", \"contact_number\", \"symptom_onset_date\",\n",
    "#         \"confirmed_date\", \"released_date\", \"deceased_date\", \"state\"\n",
    "#         ]\n",
    "\n",
    "#         # Expression to detect any change\n",
    "#         change_cond = \" OR \".join([f\"tgt.{c} <> src.{c}\" for c in compare_cols])\n",
    "\n",
    "#         (\n",
    "#             silver_tbl.alias(\"tgt\")\n",
    "#             .merge(bronze_df.alias(\"src\"), join_cond)\n",
    "#             .whenMatchedUpdate(\n",
    "#                 condition=change_cond,\n",
    "#                 set={\n",
    "#                     \"effective_end_date\": F.col(\"src.effective_start_date\"),\n",
    "#                     \"is_current\": F.lit(False)\n",
    "#                 }\n",
    "#             )\n",
    "#             .whenNotMatchedInsertAll()\n",
    "#             .execute()\n",
    "#         )\n",
    "#         # After this, you separately insert the new changed rows\n",
    "#         changed_records = bronze_df.join(silver_tbl.toDF().filter(\"is_current = false\"),join_cond, \"inner\").select(\"src.*\")\n",
    "\n",
    "#         changed_records.write.format(\"delta\").mode(\"append\").saveAsTable(silver_table_path)\n",
    "\n",
    "#     else:\n",
    "#         # First load â†’ initialize Silver table\n",
    "#         (bronze_df.write.format(\"delta\")\n",
    "#                 .mode(\"overwrite\")\n",
    "#                 .option(\"overwriteSchema\",\"true\")\n",
    "#                 .saveAsTable(silver_table_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8d7966-a19c-4e99-8182-f7e771b52eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# drop table veersadatabricks.silver.patientinfo_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "359fee67-3f1a-45d7-9158-6405222777df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Function for scd2\n",
    "run_scd2_for_patientinfo reads the Bronze patientinfo table, normalizes column names, deduplicates identical business records (keeping the latest ingestion), converts the result into SCD-2 style rows (effective_start/end, is_current), writes that dataset into a Silver Delta table (overwrite mode), then updates older rows in that Silver table to mark them as historical (is_current = False and set an effective_end_date).\n",
    "- using ingested_at as effective_start_date as ingested_at represents the time at which this snapshot became visible to our system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b485c677-57e5-4b6d-a4c8-6d96da9e00b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_scd2_for_patientinfo(catalog, bronze_schema, silver_schema):\n",
    "    # Read Bronze patientinfo\n",
    "    bronze_df = spark.read.table(f\"{catalog}.{bronze_schema}.patientinfo\")\n",
    "\n",
    "    # Basic cleaning (rename cols, lowercase)\n",
    "    for col in bronze_df.columns:\n",
    "        new_col = sanitize_name(col)\n",
    "        if new_col != col:\n",
    "            bronze_df = bronze_df.withColumnRenamed(col, new_col)\n",
    "    bronze_df = bronze_df.select([F.col(c).alias(c.lower()) for c in bronze_df.columns])\n",
    "\n",
    "    # Business columns (exclude metadata)\n",
    "    business_cols = [\n",
    "        \"patient_id\", \"sex\", \"age\", \"country\", \"province\", \"city\",\n",
    "        \"infection_case\", \"infected_by\", \"contact_number\",\n",
    "        \"symptom_onset_date\", \"confirmed_date\",\n",
    "        \"released_date\", \"deceased_date\", \"state\"\n",
    "    ]\n",
    "\n",
    "    # Dedup â†’ keep latest ingestion for identical business data\n",
    "    window_spec = Window.partitionBy(*business_cols).orderBy(F.col(\"_ingested_at\").desc())\n",
    "    bronze_df = (\n",
    "        bronze_df\n",
    "        .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"rn\") == 1)\n",
    "        .drop(\"rn\")\n",
    "    )\n",
    "\n",
    "    # Add SCD2 metadata\n",
    "    bronze_df = (\n",
    "        bronze_df\n",
    "        .withColumn(\"effective_start_date\", F.col(\"_ingested_at\"))\n",
    "        .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"is_current\", F.lit(True))\n",
    "    )\n",
    "\n",
    "    # Silver table path (catalog + schema)\n",
    "    silver_table_path = f\"{catalog}.{silver_schema}.patientinfo_silver\"\n",
    "    silver_table_fs_path = f\"/user/hive/warehouse/{silver_schema}.db/patientinfo_silver\"\n",
    "        # First load ---> creates Silver table\n",
    "    (\n",
    "        bronze_df.write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(silver_table_path)\n",
    "    )\n",
    "    #creates a delta table object from table_name, necessary to run merger operation later\n",
    "    silver_tbl = DeltaTable.forName(spark, silver_table_path)\n",
    "\n",
    "    # Window to get latest effective_start_date per patient_id\n",
    "    window_patient = Window.partitionBy(\"patient_id\").orderBy(F.col(\"effective_start_date\").desc())\n",
    "\n",
    "    silver_df = silver_tbl.toDF().withColumn(\"rn\", F.row_number().over(window_patient))\n",
    "\n",
    "    # Merge to update historical rows (rn > 1)\n",
    "    silver_tbl.alias(\"tgt\").merge(\n",
    "        silver_df.filter(F.col(\"rn\") > 1).alias(\"src\"),\n",
    "        \"tgt.patient_id = src.patient_id AND tgt.effective_start_date = src.effective_start_date\"\n",
    "    ).whenMatchedUpdate(\n",
    "        set={\"is_current\": F.lit(False), \"effective_end_date\": F.current_timestamp()}\n",
    "    ).execute()\n",
    "    print(\"SCD2 merge for patientinfo complete.\")\n",
    "\n",
    "## WORKIGN PERFECTLY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0b27922-00e8-423d-97ee-729c63ff16a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for bronze_table in bronze_tables:\n",
    "    silver_table = f\"{sanitize_name(bronze_table)}_silver\"  # auto map to Silver table name\n",
    "    print(f\"Processing {bronze_table} â†’ {silver_table}\")\n",
    "    if bronze_table == \"patientinfo\":\n",
    "        # Run SCD2 logic here (instead of overwrite), maybe run this twice\n",
    "        run_scd2_for_patientinfo(catalog, bronze_schema, silver_schema)\n",
    "    # Read from Bronze\n",
    "    else:\n",
    "        df = spark.read.table(f\"{catalog}.{bronze_schema}.{bronze_table}\")\n",
    "\n",
    "        # Basic standardization\n",
    "        # for col in df.columns:\n",
    "        #     new_col = sanitize_name(col)\n",
    "        #     if new_col != col:\n",
    "        #         df = df.withColumnRenamed(col, new_col)\n",
    "        # df = df.select([F.col(c).alias(c.lower()) for c in df.columns])\n",
    "\n",
    "        # Add ingestion metadata\n",
    "        df = add_ingestion_columns(df)  # e.g., ingested_at, is_current\n",
    "\n",
    "        # Write to Silver\n",
    "        (\n",
    "            df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .saveAsTable(f\"{catalog}.{silver_schema}.{silver_table}\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5f1362-d183-470a-b6f5-a88f39862519",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"patient_id\":90,\"sex\":90,\"age\":90,\"effective_end_date\":109,\"_source_file\":125},\"columnVisibility\":{\"infection_case\":false,\"infected_by\":false,\"contact_number\":false,\"symptom_onset_date\":false,\"confirmed_date\":false,\"released_date\":false,\"deceased_date\":false,\"state\":false,\"_ingested_at\":false,\"country\":false,\"province\":false,\"city\":false,\"effective_start_date\":false}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757989046942}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1757989087020}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757932053102}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df = spark.table(\"veersadatabricks.silver.patientinfo_silver\")\n",
    "print(df.count())\n",
    "df_bronze = spark.table(\"veersadatabricks.bronze.patientinfo\")\n",
    "#print(df_bronze.count())\n",
    "df.filter(col(\"patient_id\")==1000000001).display()\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1135fb0d-14d1-49ad-9b40-04092da11b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# patientinfo = spark.read.table(\"veersadatabricks.silver.patientinfo_silver\")\n",
    "# patientinfo.printSchema()\n",
    "# print(patientinfo.count())\n",
    "# case_table_silver = spark.read.table(\"veersadatabricks.Silver.case_table_silver\")\n",
    "# case_table_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "28f584a7-21be-4b92-8c64-33c134905b00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# case_bronze = spark.table(\"veersadatabricks.bronze.case_table\")\n",
    "# #case_bronze.printSchema()\n",
    "# case_silver = (\n",
    "#     case_bronze\n",
    "#     .withColumnRenamed(\"_case_id\", \"caseId\")\n",
    "#     .withColumnRenamed(\"confirmed\", \"confirmedCases\")\n",
    "#     .withColumn(\"confirmedCases\", col(\"confirmedCases\").cast(\"int\"))\n",
    "#     .withColumn(\"city\", trim(col(\"city\")))\n",
    "#     .filter(\n",
    "#         col(\"caseId\").isNotNull() &\n",
    "#         col(\"confirmedCases\").isNotNull() \n",
    "#         #&\n",
    "#         #(col(\"city\").isNotNull() & (col(\"city\") != \"-\"))\n",
    "#     )\n",
    "#     .dropDuplicates()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "28173157-492a-4b14-8d5b-6c376718f2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def run_scd2_for_patientinfo(catalog, bronze_schema, silver_schema):\n",
    "#     # Read and process bronze data (same as before)\n",
    "#     bronze_df = spark.read.table(f\"{catalog}.{bronze_schema}.patientinfo\")\n",
    "    \n",
    "#     # ... (same cleaning and dedup logic as original)\n",
    "#     for col in bronze_df.columns:\n",
    "#         new_col = sanitize_name(col)\n",
    "#         if new_col != col:\n",
    "#             bronze_df = bronze_df.withColumnRenamed(col, new_col)\n",
    "#     bronze_df = bronze_df.select([F.col(c).alias(c.lower()) for c in bronze_df.columns])\n",
    "\n",
    "#     business_cols = [\"patient_id\", \"sex\", \"age\", \"country\", \"province\", \"city\", \"infection_case\", \"infected_by\", \"contact_number\", \"symptom_onset_date\", \"confirmed_date\", \"released_date\", \"deceased_date\", \"state\"]\n",
    "\n",
    "#     window_spec = Window.partitionBy(*business_cols).orderBy(F.col(\"_ingested_at\").desc())\n",
    "#     # Add SCD2 metadata\n",
    "#     bronze_df = (\n",
    "#         bronze_df\n",
    "#         .withColumn(\"effective_start_date\", F.col(\"_ingested_at\"))\n",
    "#         .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\"))\n",
    "#         .withColumn(\"is_current\", F.lit(True))\n",
    "#     )\n",
    "\n",
    "#     silver_table_path = f\"{catalog}.{silver_schema}.patientinfo_silver\"\n",
    "\n",
    "#     if spark.catalog.tableExists(silver_table_path):\n",
    "#         silver_tbl = DeltaTable.forName(spark, silver_table_path)\n",
    "        \n",
    "#         # Get current active records from silver\n",
    "#         current_silver = spark.read.table(silver_table_path).filter(F.col(\"is_current\") == True)\n",
    "        \n",
    "#         # Identify what's changed by comparing business columns\n",
    "#         changed_patients = (\n",
    "#             bronze_df.alias(\"bronze\")\n",
    "#             .join(current_silver.alias(\"silver\"), \"patient_id\", \"inner\")\n",
    "#             .filter(\n",
    "#                 reduce(lambda x, y: x | y, [\n",
    "#                     ~F.col(f\"bronze.{col}\").eqNullSafe(F.col(f\"silver.{col}\")) \n",
    "#                     for col in business_cols if col != \"patient_id\"\n",
    "#                 ])\n",
    "#             )\n",
    "#             .select(\"bronze.patient_id\")\n",
    "#             .distinct()\n",
    "#         )\n",
    "        \n",
    "#         changed_patient_ids = [row.patient_id for row in changed_patients.collect()]\n",
    "        \n",
    "#         if changed_patient_ids:\n",
    "#             print(f\"Found {len(changed_patient_ids)} changed patients\")\n",
    "            \n",
    "#             # Close old versions\n",
    "#             silver_tbl.update(\n",
    "#                 condition=F.col(\"patient_id\").isin(changed_patient_ids) & (F.col(\"is_current\") == True),\n",
    "#                 set={\n",
    "#                     \"effective_end_date\": F.current_timestamp(),\n",
    "#                     \"is_current\": F.lit(False)\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # Insert new versions (changed records)\n",
    "#             new_changed_records = bronze_df.filter(F.col(\"patient_id\").isin(changed_patient_ids))\n",
    "#             new_changed_records.write.format(\"delta\").mode(\"append\").saveAsTable(silver_table_path)\n",
    "        \n",
    "#         # Insert completely new records\n",
    "#         existing_patient_ids = current_silver.select(\"patient_id\").distinct().rdd.map(lambda x: x[0]).collect()\n",
    "#         new_records = bronze_df.filter(~F.col(\"patient_id\").isin(existing_patient_ids))\n",
    "        \n",
    "#         if new_records.count() > 0:\n",
    "#             print(f\"Inserting {new_records.count()} new patient records\")\n",
    "#             new_records.write.format(\"delta\").mode(\"append\").saveAsTable(silver_table_path)\n",
    "    \n",
    "#     else:\n",
    "#         # First load\n",
    "#         bronze_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_table_path)\n",
    "\n",
    "#     print(\"âœ… SCD2 processing complete.\")            cols_to_drop = [f\"{col}_changed\" for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c185e12e-4e29-410c-bfef-21c16b53ee7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# case_silver.write.format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"path\", f\"abfss://{container}@{storage_account}.dfs.core.windows.net/Silver/your_table\") \\\n",
    "#     .saveAsTable(\"veersadatabricks.Silver.case_silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9fc500fd-3df6-47af-ac17-fc063f92bf92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from delta.tables import DeltaTable\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# def run_scd2_for_patientinfo(catalog, bronze_schema, silver_schema):\n",
    "#     # Read Bronze patientinfo\n",
    "#     bronze_df = spark.read.table(f\"{catalog}.{bronze_schema}.patientinfo\")\n",
    "\n",
    "#     # Basic cleaning (rename cols, lowercase)\n",
    "#     for col in bronze_df.columns:\n",
    "#         new_col = sanitize_name(col)\n",
    "#         if new_col != col:\n",
    "#             bronze_df = bronze_df.withColumnRenamed(col, new_col)\n",
    "#     bronze_df = bronze_df.select([F.col(c).alias(c.lower()) for c in bronze_df.columns])\n",
    "\n",
    "#     # Business columns (exclude metadata)\n",
    "#     business_cols = [\n",
    "#         \"patient_id\", \"sex\", \"age\", \"country\", \"province\", \"city\",\n",
    "#         \"infection_case\", \"infected_by\", \"contact_number\",\n",
    "#         \"symptom_onset_date\", \"confirmed_date\",\n",
    "#         \"released_date\", \"deceased_date\", \"state\"\n",
    "#     ]\n",
    "\n",
    "#     # Dedup â†’ keep latest ingestion for identical business data\n",
    "#     window_spec = Window.partitionBy(*business_cols).orderBy(F.col(\"_ingested_at\").desc())\n",
    "#     bronze_df = (\n",
    "#         bronze_df\n",
    "#         .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "#         .filter(F.col(\"rn\") == 1)\n",
    "#         .drop(\"rn\")\n",
    "#     )\n",
    "\n",
    "#     # Add SCD2 metadata\n",
    "#     bronze_df = (\n",
    "#         bronze_df\n",
    "#         .withColumn(\"effective_start_date\", F.col(\"_ingested_at\"))\n",
    "#         .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\"))\n",
    "#         .withColumn(\"is_current\", F.lit(True))\n",
    "#     )\n",
    "\n",
    "#     # Silver table path (catalog + schema)\n",
    "#     silver_table_path = f\"{catalog}.{silver_schema}.patientinfo_silver\"\n",
    "#     silver_table_fs_path = f\"/user/hive/warehouse/{silver_schema}.db/patientinfo_silver\"\n",
    "\n",
    "#     if spark.catalog.tableExists(silver_table_path):\n",
    "#         silver_tbl = DeltaTable.forName(spark, silver_table_path)\n",
    "#         print(\"in here!!!!!!!\")\n",
    "#         # Natural key\n",
    "#         join_cond = \"tgt.patient_id = src.patient_id AND tgt.is_current = true\"\n",
    "\n",
    "#         # Change detection (null-safe)\n",
    "#         compare_cols = [c for c in business_cols if c != \"patient_id\"]\n",
    "#         # compare_cols.append(F.col(\"src._ingested_at\") > F.col(\"tgt.effective_start_date\"))\n",
    "\n",
    "#         # # Combine all conditions with OR\n",
    "#         # from functools import reduce\n",
    "#         # from operator import or_\n",
    "#         # change_cond = reduce(or_, compare_cols)\n",
    "#         # #change_cond = \" OR \".join([f\"NOT (tgt.{c} <=> src.{c})\" for c in compare_cols])\n",
    "\n",
    "#         compare_exprs = [~(F.expr(f\"tgt.{c} <=> src.{c}\")) for c in compare_cols]\n",
    "\n",
    "#         # If you also want to include _ingested_at check\n",
    "#         compare_exprs.append(F.col(\"src._ingested_at\") > F.col(\"tgt.effective_start_date\"))\n",
    "\n",
    "#         # Combine all conditions into a single OR expression\n",
    "#         change_cond = reduce(or_, compare_exprs)\n",
    "\n",
    "#         (\n",
    "#             silver_tbl.alias(\"tgt\")\n",
    "#             .merge(bronze_df.alias(\"src\"), join_cond)\n",
    "#             .whenMatchedUpdate(\n",
    "#                 condition=change_cond,\n",
    "#                 set={\n",
    "#                     \"effective_end_date\": F.col(\"src.effective_start_date\"),\n",
    "#                     \"is_current\": F.lit(False)\n",
    "#                 }\n",
    "#             )\n",
    "#             .whenNotMatchedInsertAll()\n",
    "#             .execute()\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         # First load â†’ create Silver table\n",
    "#         (\n",
    "#             bronze_df.write.format(\"delta\")\n",
    "#             .mode(\"overwrite\")\n",
    "#             .option(\"overwriteSchema\", \"true\")\n",
    "#             .saveAsTable(silver_table_path)\n",
    "#         )\n",
    "\n",
    "#     print(\"âœ… SCD2 merge for patientinfo complete.\")\n",
    "\n",
    "# ## WORKIGN PERFECTLY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9d23edb0-0730-4b3a-95d6-e21636976d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from delta.tables import DeltaTable\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# def run_scd2_for_patientinfo(catalog, bronze_schema, silver_schema):\n",
    "#     # Read Bronze patientinfo\n",
    "#     bronze_df = spark.read.table(f\"{catalog}.{bronze_schema}.patientinfo\")\n",
    "\n",
    "#     # Basic cleaning (rename cols, lowercase)\n",
    "#     for col in bronze_df.columns:\n",
    "#         new_col = sanitize_name(col)\n",
    "#         if new_col != col:\n",
    "#             bronze_df = bronze_df.withColumnRenamed(col, new_col)\n",
    "#     bronze_df = bronze_df.select([F.col(c).alias(c.lower()) for c in bronze_df.columns])\n",
    "\n",
    "#     # Business columns\n",
    "#     business_cols = [\n",
    "#         \"patient_id\", \"sex\", \"age\", \"country\", \"province\", \"city\",\n",
    "#         \"infection_case\", \"infected_by\", \"contact_number\",\n",
    "#         \"symptom_onset_date\", \"confirmed_date\",\n",
    "#         \"released_date\", \"deceased_date\", \"state\"\n",
    "#     ]\n",
    "\n",
    "#     # Dedup â†’ keep latest ingestion for identical business data\n",
    "#     window_spec = Window.partitionBy(*business_cols).orderBy(F.col(\"_ingested_at\").desc())\n",
    "#     bronze_df = (\n",
    "#         bronze_df\n",
    "#         .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "#         .filter(F.col(\"rn\") == 1)\n",
    "#         .drop(\"rn\")\n",
    "#     )\n",
    "\n",
    "#     # Add SCD2 metadata\n",
    "#     bronze_df = (\n",
    "#         bronze_df\n",
    "#         .withColumn(\"effective_start_date\", F.col(\"_ingested_at\"))\n",
    "#         .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\"))\n",
    "#         .withColumn(\"is_current\", F.lit(True))\n",
    "#     )\n",
    "\n",
    "#     # Silver table path\n",
    "#     silver_table_path = f\"{catalog}.{silver_schema}.patientinfo_silver\"\n",
    "#     silver_table_fs_path = f\"/user/hive/warehouse/{silver_schema}.db/patientinfo_silver\"\n",
    "#     compare_cols = [c for c in business_cols if c != \"patient_id\"]\n",
    "#     if DeltaTable.isDeltaTable(spark, silver_table_fs_path):\n",
    "#         silver_tbl = DeltaTable.forName(spark, silver_table_fs_path)\n",
    "\n",
    "#         join_cond = \"tgt.patient_id = src.patient_id AND tgt.is_current = true\"\n",
    "\n",
    "#         # compare_cols = [c for c in business_cols if c != \"patient_id\"]\n",
    "#         # change_cond = \" OR \".join([f\"NOT (tgt.{c} <=> src.{c})\" for c in compare_cols])\n",
    "#         print(\"Got here!!!!!!!!!!!!!!\")\n",
    "#         compare_exprs = [f\"NOT (tgt.{c} <=> src.{c})\" for c in compare_cols]\n",
    "#        # change_cond_col = reduce(or_, compare_exprs)  # combine with OR\n",
    "#         change_cond_col = \" OR \".join(compare_exprs)\n",
    "#         (\n",
    "#             silver_tbl.alias(\"tgt\")\n",
    "#             .merge(bronze_df.alias(\"src\"), \"tgt.patient_id = src.patient_id AND tgt.is_current = true\")\n",
    "#             # 1. Close existing record if any change\n",
    "#             .whenMatchedUpdate(\n",
    "#                 condition=change_cond_col,\n",
    "#                 set={\n",
    "#                     \"effective_end_date\": F.col(\"src.effective_start_date\"),\n",
    "#                     \"is_current\": F.lit(False)\n",
    "#                 }\n",
    "#             )\n",
    "#             # 2. Always insert new record (if not exact duplicate)\n",
    "#             .whenNotMatchedInsertAll()\n",
    "#             .whenMatchedInsert(\n",
    "#                 condition=change_cond_col,\n",
    "#                 values={c: F.col(f\"src.{c}\") for c in bronze_df.columns}\n",
    "#             )\n",
    "#             .execute()\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         (\n",
    "#             bronze_df.write.format(\"delta\")\n",
    "#             .mode(\"overwrite\")\n",
    "#             .option(\"overwriteSchema\", \"true\")\n",
    "#             .saveAsTable(silver_table_path)\n",
    "#         )\n",
    "\n",
    "#     print(\"âœ… SCD2 merge for patientinfo complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8745f654-c96c-46e1-b3be-39f5f2e98973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from delta.tables import DeltaTable\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# def run_scd2_for_patientinfo(catalog, bronze_schema, silver_schema):\n",
    "#     # Read Bronze patientinfo\n",
    "#     bronze_df = spark.read.table(f\"{catalog}.{bronze_schema}.patientinfo\")\n",
    "\n",
    "#     # Basic cleaning (rename cols, lowercase)\n",
    "#     for col in bronze_df.columns:\n",
    "#         new_col = sanitize_name(col)\n",
    "#         if new_col != col:\n",
    "#             bronze_df = bronze_df.withColumnRenamed(col, new_col)\n",
    "#     bronze_df = bronze_df.select([F.col(c).alias(c.lower()) for c in bronze_df.columns])\n",
    "\n",
    "#     # Business columns (exclude metadata)\n",
    "#     business_cols = [\n",
    "#         \"patient_id\", \"sex\", \"age\", \"country\", \"province\", \"city\",\n",
    "#         \"infection_case\", \"infected_by\", \"contact_number\",\n",
    "#         \"symptom_onset_date\", \"confirmed_date\",\n",
    "#         \"released_date\", \"deceased_date\", \"state\"\n",
    "#     ]\n",
    "\n",
    "#     # Dedup â†’ keep latest ingestion for identical business data\n",
    "#     window_spec = Window.partitionBy(*business_cols).orderBy(F.col(\"_ingested_at\").desc())\n",
    "#     bronze_df = (\n",
    "#         bronze_df\n",
    "#         .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "#         .filter(F.col(\"rn\") == 1)\n",
    "#         .drop(\"rn\")\n",
    "#     )\n",
    "\n",
    "#     # Add SCD2 metadata\n",
    "#     bronze_df = (\n",
    "#         bronze_df\n",
    "#         .withColumn(\"effective_start_date\", F.col(\"_ingested_at\"))\n",
    "#         .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\"))\n",
    "#         .withColumn(\"is_current\", F.lit(True))\n",
    "#     )\n",
    "\n",
    "#     # Silver table path\n",
    "#     silver_table_path = f\"{catalog}.{silver_schema}.patientinfo_silver\"\n",
    "\n",
    "#     if spark.catalog.tableExists(silver_table_path):\n",
    "#         print(\"Processing existing silver table...\")\n",
    "        \n",
    "#         # Read current silver data\n",
    "#         silver_df = spark.read.table(silver_table_path)\n",
    "#         current_silver = silver_df.filter(F.col(\"is_current\") == True)\n",
    "        \n",
    "#         # Find what records have changed by comparing business columns only\n",
    "#         compare_cols = [c for c in business_cols if c != \"patient_id\"]\n",
    "        \n",
    "#         # Get records that exist in both bronze and current silver\n",
    "#         existing_patients = (\n",
    "#             bronze_df.alias(\"bronze\")\n",
    "#             .join(current_silver.alias(\"silver\"), \"patient_id\", \"inner\")\n",
    "#             .select(\n",
    "#                 \"bronze.*\",\n",
    "#                 # Add flags to check if data changed\n",
    "#                 *[\n",
    "#                     (~F.col(f\"bronze.{col}\").eqNullSafe(F.col(f\"silver.{col}\"))).alias(f\"{col}_changed\")\n",
    "#                     for col in compare_cols\n",
    "#                 ]\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#         # Create a single change flag\n",
    "#         change_conditions = [F.col(f\"{col}_changed\") for col in compare_cols]\n",
    "#         combined_change_flag = change_conditions[0]\n",
    "#         for condition in change_conditions[1:]:\n",
    "#             combined_change_flag = combined_change_flag | condition\n",
    "            \n",
    "#         existing_patients = existing_patients.withColumn(\"has_changes\", combined_change_flag)\n",
    "        \n",
    "#         # Separate changed vs unchanged patients\n",
    "#         changed_patients = existing_patients.filter(F.col(\"has_changes\") == True)\n",
    "#         unchanged_patients = existing_patients.filter(F.col(\"has_changes\") == False)\n",
    "#         new_patients = bronze_df.alias(\"bronze\").join(\n",
    "#             current_silver.alias(\"silver\"), \"patient_id\", \"left_anti\"\n",
    "#         )\n",
    "        \n",
    "#         print(f\"Changed patients: {changed_patients.count()}\")\n",
    "#         print(f\"Unchanged patients: {unchanged_patients.count()}\")  \n",
    "#         print(f\"New patients: {new_patients.count()}\")\n",
    "        \n",
    "#         if changed_patients.count() > 0:\n",
    "#             # Get the patient IDs that changed\n",
    "#             changed_patient_ids = [row.patient_id for row in changed_patients.select(\"patient_id\").distinct().collect()]\n",
    "            \n",
    "#             # Step 1: Close the old versions (set is_current = False, effective_end_date)\n",
    "#             silver_tbl = DeltaTable.forName(spark, silver_table_path)\n",
    "            \n",
    "#             # Get the new effective_start_date for each changed patient\n",
    "#             patient_new_dates = {\n",
    "#                 row.patient_id: row.effective_start_date \n",
    "#                 for row in changed_patients.select(\"patient_id\", \"effective_start_date\").collect()\n",
    "#             }\n",
    "            \n",
    "#             # Update old records one by one to avoid confusion\n",
    "#             for patient_id in changed_patient_ids:\n",
    "#                 new_start_date = patient_new_dates[patient_id]\n",
    "#                 silver_tbl.update(\n",
    "#                     condition=(F.col(\"patient_id\") == patient_id) & (F.col(\"is_current\") == True),\n",
    "#                     set={\n",
    "#                         \"effective_end_date\": F.lit(new_start_date),\n",
    "#                         \"is_current\": F.lit(False)\n",
    "#                     }\n",
    "#                 )\n",
    "            \n",
    "#             # Step 2: Insert the new versions (drop the helper columns first)\n",
    "#             cols_to_drop = [f\"{col}_changed\" for col in compare_cols] + [\"has_changes\"]\n",
    "#             new_versions = changed_patients.drop(*cols_to_drop)\n",
    "            \n",
    "#             new_versions.write.format(\"delta\").mode(\"append\").saveAsTable(silver_table_path)\n",
    "#             print(f\"Inserted {new_versions.count()} new versions of changed records\")\n",
    "        \n",
    "#         # Insert completely new patients\n",
    "#         if new_patients.count() > 0:\n",
    "#             new_patients.write.format(\"delta\").mode(\"append\").saveAsTable(silver_table_path)\n",
    "#             print(f\"Inserted {new_patients.count()} completely new patient records\")\n",
    "            \n",
    "#     else:\n",
    "#         # First load â†’ create Silver table\n",
    "#         print(\"Creating new silver table...\")\n",
    "#         (\n",
    "#             bronze_df.write.format(\"delta\")\n",
    "#             .mode(\"overwrite\")\n",
    "#             .option(\"overwriteSchema\", \"true\")\n",
    "#             .saveAsTable(silver_table_path)\n",
    "#         )\n",
    "\n",
    "#     print(\"âœ… SCD2 merge for patientinfo complete.\")\n",
    "\n",
    "# # Corrected approach using two separate operations\n",
    "# def run_scd2_for_patientinfo_clean(catalog, bronze_schema, silver_schema):\n",
    "#     # Read and clean bronze data (same as above)\n",
    "#     bronze_df = spark.read.table(f\"{catalog}.{bronze_schema}.patientinfo\")\n",
    "    \n",
    "#     # Basic cleaning (rename cols, lowercase)\n",
    "#     for col in bronze_df.columns:\n",
    "#         new_col = sanitize_name(col)\n",
    "#         if new_col != col:\n",
    "#             bronze_df = bronze_df.withColumnRenamed(col, new_col)\n",
    "#     bronze_df = bronze_df.select([F.col(c).alias(c.lower()) for c in bronze_df.columns])\n",
    "\n",
    "#     # Business columns (exclude metadata)\n",
    "#     business_cols = [\n",
    "#         \"patient_id\", \"sex\", \"age\", \"country\", \"province\", \"city\",\n",
    "#         \"infection_case\", \"infected_by\", \"contact_number\",\n",
    "#         \"symptom_onset_date\", \"confirmed_date\",\n",
    "#         \"released_date\", \"deceased_date\", \"state\"\n",
    "#     ]\n",
    "\n",
    "#     # Dedup â†’ keep latest ingestion for identical business data\n",
    "#     window_spec = Window.partitionBy(*business_cols).orderBy(F.col(\"_ingested_at\").desc())\n",
    "#     bronze_df = (\n",
    "#         bronze_df\n",
    "#         .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "#         .filter(F.col(\"rn\") == 1)\n",
    "#         .drop(\"rn\")\n",
    "#     )\n",
    "    \n",
    "#     # Add SCD2 metadata\n",
    "#     bronze_df = (\n",
    "#         bronze_df\n",
    "#         .withColumn(\"effective_start_date\", F.col(\"_ingested_at\"))\n",
    "#         .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\"))\n",
    "#         .withColumn(\"is_current\", F.lit(True))\n",
    "#     )\n",
    "\n",
    "#     silver_table_path = f\"{catalog}.{silver_schema}.patientinfo_silver\"\n",
    "\n",
    "#     if spark.catalog.tableExists(silver_table_path):\n",
    "#         # STEP 1: Clear any existing duplicates first\n",
    "#         print(\"Cleaning existing table...\")\n",
    "#         silver_df = spark.read.table(silver_table_path)\n",
    "        \n",
    "#         # Keep only the latest version of each patient (by effective_start_date)\n",
    "#         window_spec_clean = Window.partitionBy(\"patient_id\").orderBy(F.col(\"effective_start_date\").desc())\n",
    "#         cleaned_silver = (\n",
    "#             silver_df\n",
    "#             .withColumn(\"rn\", F.row_number().over(window_spec_clean))\n",
    "#             .filter(F.col(\"rn\") == 1)\n",
    "#             .drop(\"rn\")\n",
    "#             .withColumn(\"is_current\", F.lit(True))\n",
    "#             .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\"))\n",
    "#         )\n",
    "        \n",
    "#         # Overwrite with cleaned data\n",
    "#         cleaned_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table_path)\n",
    "#         print(\"Table cleaned - removed duplicates\")\n",
    "        \n",
    "#         # STEP 2: Now do the SCD2 process with two operations\n",
    "#         silver_tbl = DeltaTable.forName(spark, silver_table_path)\n",
    "        \n",
    "#         compare_cols = [c for c in business_cols if c != \"patient_id\"]\n",
    "#         change_cond = \" OR \".join([f\"NOT (tgt.{c} <=> src.{c})\" for c in compare_cols])\n",
    "\n",
    "#         # Operation 1: Close old records when changes detected\n",
    "#         print(\"Step 1: Closing old records...\")\n",
    "#         (\n",
    "#             silver_tbl.alias(\"tgt\")\n",
    "#             .merge(bronze_df.alias(\"src\"), \"tgt.patient_id = src.patient_id AND tgt.is_current = true\")\n",
    "#             .whenMatchedUpdate(\n",
    "#                 condition=change_cond,\n",
    "#                 set={\n",
    "#                     \"effective_end_date\": \"src.effective_start_date\",\n",
    "#                     \"is_current\": \"false\"\n",
    "#                 }\n",
    "#             )\n",
    "#             .execute()\n",
    "#         )\n",
    "        \n",
    "#         # Operation 2: Insert all records (new + changed versions)\n",
    "#         print(\"Step 2: Inserting new records...\")\n",
    "#         (\n",
    "#             silver_tbl.alias(\"tgt\")\n",
    "#             .merge(bronze_df.alias(\"src\"), \"tgt.patient_id = src.patient_id AND tgt.is_current = true\")\n",
    "#             .whenNotMatchedInsertAll()\n",
    "#             .execute()\n",
    "#         )\n",
    "        \n",
    "#     else:\n",
    "#         # First load\n",
    "#         print(\"Creating new silver table...\")\n",
    "#         bronze_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_table_path)\n",
    "\n",
    "#     print(\"âœ… SCD2 processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6891981014863519,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
