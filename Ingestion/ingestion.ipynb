{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b2192b2-0c96-4f22-a3ca-d309dc4b4bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Downloading essential python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2f6d5504-c6ca-4669-be2a-f316c52e4f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06bc3f96-d77f-4791-bc57-b61a20e2de8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d7e143-86d6-448f-bced-d32f9a574947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c650cf-75b3-425c-a46d-23052a92eb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading essential data from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2bbfef0-91db-4abd-b4c3-5d50b1a34a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba3669bb-5979-451b-9a0b-79cdda631700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = os.getenv(\"catalog\")\n",
    "schema = os.getenv(\"bronze_schema\")\n",
    "container = os.getenv(\"container\")\n",
    "storage_account = os.getenv(\"storage_account\")\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb48ba76-6919-46a2-8a71-807d6219aa1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bronze Path Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16340bcc-453a-48cb-a6cb-3fbf38b2cf43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_path=f\"abfss://{container}@{storage_account}.dfs.core.windows.net/Bronze/\"\n",
    "# print(bronze_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9730345-f3fd-43c0-9dfe-010ea29a6552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ensuring schema exists\n",
    "- DataFrame[]: this is the standard return type for all sql commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94050a53-648a-4b14-afeb-b84aedae82d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7fe4ae3-b371-436b-8ae1-6a51347730d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating function for ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1181b0a9-a51e-4d96-812d-6cea81bcbc28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RESERVED_KEYWORDS = {\"case\", \"select\", \"from\", \"table\"}  # expand as needed\n",
    "\n",
    "def sanitize_name(name):\n",
    "    \"\"\"\n",
    "    Replace invalid characters and handle reserved keywords.\n",
    "    \"\"\"\n",
    "    # Replace invalid characters\n",
    "    sanitized = re.sub(r\"[ ,;{}()\\n\\t=]\", \"_\", name)\n",
    "    # Handle reserved keywords\n",
    "    if sanitized.lower() in RESERVED_KEYWORDS:\n",
    "        sanitized = sanitized + \"_table\"\n",
    "    return sanitized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc1028f-e162-4e99-9697-146cb6c855ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Running Ingestion Function\n",
    "- bronze layer in medallion architecture is used for raw data ingestion\n",
    "- catalog and schema are passed for final delta table creation from raw csv files in bronze layer\n",
    "#### For is_file_already_ingested:\n",
    "- DeltaTable.isDeltaTable checks the folder path for _delta_log/ metadata. If present, it identifies the path as a valid Delta table.\n",
    "- ingestion meta data is stored in a delta table as it has ACID properties and prevents deduplication of data\n",
    "- .limit(1).count() is slightly more efficient than counting all rows because it stops scanning after finding 1 matching row. - basically checking if a log row exists for that table\n",
    "- if no delta table exist yet, the functions assumes files have not been ingested and returns False\n",
    "\n",
    "#### For log_ingestion:\n",
    "- creating a row whcih hold file log values like table name, file name, file modification time\n",
    "- this row is then converted into a dataframe with the schema as depicted\n",
    "- it then checks, if ingestion_log_path delta table exists, if it does then, file log is appended and if not then the ingestion log_path table is created\n",
    "\n",
    "#### Main code\n",
    "- checks for files in each subfolder\n",
    "- separate case to prevent multiple appends for patientinfo table\n",
    "- plain overwrite for remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b7da5eae-464e-4dd3-9a7d-36590ed72f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def ingest_latest_files(bronze_path, catalog, schema):\n",
    "#     def is_file_already_ingested(table_name, file_name):\n",
    "#         if DeltaTable.isDeltaTable(spark, ingestion_log_path):\n",
    "#             log_df = spark.read.format(\"delta\").load(ingestion_log_path)\n",
    "#             return log_df.filter(\n",
    "#                 (F.col(\"table_name\") == table_name) & (F.col(\"file_name\") == file_name)\n",
    "#             ).limit(1).count() > 0\n",
    "#         return False\n",
    "\n",
    "#     def log_ingestion(table_name, file_name, file_mod_iso):\n",
    "#         row = [(table_name, file_name, file_mod_iso, datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"))]\n",
    "#         log_df = spark.createDataFrame(row, schema=[\"table_name\",\"file_name\",\"file_mod_ts\",\"logged_at\"])\n",
    "#         if DeltaTable.isDeltaTable(spark, ingestion_log_path):\n",
    "#             log_df.write.format(\"delta\").mode(\"append\").save(ingestion_log_path)\n",
    "#         else:\n",
    "#             log_df.write.format(\"delta\").mode(\"overwrite\").save(ingestion_log_path)\n",
    "\n",
    "#     # List all subfolders inside Bronze\n",
    "#     subfolders = [f.path for f in dbutils.fs.ls(bronze_path) if f.isDir()]\n",
    "\n",
    "#     for folder in subfolders:\n",
    "#         table_name = os.path.basename(folder.rstrip(\"/\")).lower()\n",
    "#         output_path = f\"{bronze_path}delta/{table_name}/\"\n",
    "\n",
    "#         # List files in the subfolder\n",
    "#         files = [f for f in dbutils.fs.ls(folder) if f.name.endswith(\".csv\")]\n",
    "#         if not files:\n",
    "#             print(f\"No files found in {folder}\")\n",
    "#             continue\n",
    "\n",
    "#         if table_name == \"patientinfo\":\n",
    "#             #For PatientInfo → append new files only\n",
    "#             for file in files:\n",
    "#                 file_mod_iso = datetime.utcfromtimestamp(file.modificationTime / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#                 if is_file_already_ingested(table_name, file.name):\n",
    "#                     print(f\"Skipping {file.name} — already ingested.\")\n",
    "#                     continue\n",
    "\n",
    "#                 df_new = (\n",
    "#                     spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(file.path)\n",
    "#                     .withColumn(\"_source_file\", F.lit(file.name))\n",
    "#                     .withColumn(\"_ingested_at\", F.to_timestamp(F.lit(file_mod_iso)))\n",
    "#                 )\n",
    "\n",
    "#                 df_new.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").save(output_path)\n",
    "#                 log_ingestion(table_name, file.name, file_mod_iso)\n",
    "\n",
    "#                 spark.sql(f\"\"\"\n",
    "#                   CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table_name}\n",
    "#                   USING DELTA\n",
    "#                   LOCATION '{output_path}'\n",
    "#                 \"\"\")\n",
    "#                 spark.sql(f\"ALTER TABLE {catalog}.{schema}.{table_name} SET LOCATION '{output_path}'\")\n",
    "#                 print(f\"Appended {file.name} into {table_name}\")\n",
    "\n",
    "#         else:\n",
    "#             # For all other tables → full overwrite\n",
    "#             latest_file = sorted(files, key=lambda f: f.modificationTime)[-1]\n",
    "#             df = (\n",
    "#                 spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(latest_file.path)\n",
    "#                 .withColumn(\"_source_file\", F.lit(latest_file.name))\n",
    "#                 .withColumn(\"_ingested_at\", F.current_timestamp())\n",
    "#             )\n",
    "#             for c in df.columns:\n",
    "#                 new_col = sanitize_name(c)\n",
    "#                 if new_col != c:\n",
    "#                     df = df.withColumnRenamed(c, new_col)\n",
    "\n",
    "#             df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(output_path)\n",
    "\n",
    "#             spark.sql(f\"\"\"\n",
    "#               CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table_name}\n",
    "#               USING DELTA\n",
    "#               LOCATION '{output_path}'\n",
    "#             \"\"\")\n",
    "\n",
    "#             print(f\"Overwrote {table_name} with {latest_file.name}\")\n",
    "\n",
    "#     print(\"Bronze ingestion run complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b793c979-58ae-4bc8-a54b-2f7c628102f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def ingest_latest_files(bronze_path, catalog, schema):\n",
    "#     def is_file_already_ingested(table_name, file_name):\n",
    "#         if DeltaTable.isDeltaTable(spark, ingestion_log_path):\n",
    "#             log_df = spark.read.format(\"delta\").load(ingestion_log_path)\n",
    "#             return log_df.filter(\n",
    "#                 (F.col(\"table_name\") == table_name) & (F.col(\"file_name\") == file_name)\n",
    "#             ).limit(1).count() > 0\n",
    "#         return False\n",
    "\n",
    "#     def log_ingestion(table_name, file_name, file_mod_iso):\n",
    "#         row = [(table_name, file_name, file_mod_iso, datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"))]\n",
    "#         log_df = spark.createDataFrame(row, schema=[\"table_name\",\"file_name\",\"file_mod_ts\",\"logged_at\"])\n",
    "#         if DeltaTable.isDeltaTable(spark, ingestion_log_path):\n",
    "#             log_df.write.format(\"delta\").mode(\"append\").save(ingestion_log_path)\n",
    "#         else:\n",
    "#             log_df.write.format(\"delta\").mode(\"overwrite\").save(ingestion_log_path)\n",
    "\n",
    "#     # List all subfolders inside Bronze\n",
    "#     subfolders = [f.path for f in dbutils.fs.ls(bronze_path) if f.isDir()]\n",
    "\n",
    "#     for folder in subfolders:\n",
    "#         table_name = os.path.basename(folder.rstrip(\"/\")).lower()\n",
    "#         output_path = f\"{bronze_path}delta/{table_name}/\"\n",
    "\n",
    "#         # List files in the subfolder\n",
    "#         files = [f for f in dbutils.fs.ls(folder) if f.name.endswith(\".csv\")]\n",
    "#         if not files:\n",
    "#             print(f\"No files found in {folder}\")\n",
    "#             continue\n",
    "\n",
    "#         if table_name == \"patientinfo\":\n",
    "#             #For PatientInfo → append new files only\n",
    "#             for file in files:\n",
    "#                 file_mod_iso = datetime.utcfromtimestamp(file.modificationTime / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#                 if is_file_already_ingested(table_name, file.name):\n",
    "#                     print(f\"Skipping {file.name} — already ingested.\")\n",
    "#                     continue\n",
    "\n",
    "#                 df_new = (\n",
    "#                     spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(file.path)\n",
    "#                     .withColumn(\"_source_file\", F.lit(file.name))\n",
    "#                     .withColumn(\"_ingested_at\", F.to_timestamp(F.lit(file_mod_iso)))\n",
    "#                 )\n",
    "\n",
    "#                 df_new.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").save(output_path)\n",
    "#                 log_ingestion(table_name, file.name, file_mod_iso)\n",
    "\n",
    "#                 spark.sql(f\"\"\"\n",
    "#                   CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table_name}\n",
    "#                   USING DELTA\n",
    "#                   LOCATION '{output_path}'\n",
    "#                 \"\"\")\n",
    "#                 spark.sql(f\"ALTER TABLE {catalog}.{schema}.{table_name} SET LOCATION '{output_path}'\")\n",
    "#                 print(f\"Appended {file.name} into {table_name}\")\n",
    "\n",
    "#         else:\n",
    "#             # For all other tables → full overwrite\n",
    "#             #latest_file = sorted(files, key=lambda f: f.modificationTime)[-1]\n",
    "#             for file in files:\n",
    "#                 file_mod_iso = datetime.utcfromtimestamp(file.modificationTime / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#                 if is_file_already_ingested(table_name,file.name):\n",
    "#                     print(f\"Already ingested, skipping {file.name}\")\n",
    "#                     continue\n",
    "                \n",
    "#                 df = (\n",
    "#                     spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(file.path)\n",
    "#                     .withColumn(\"_source_file\", F.lit(file.name))\n",
    "#                     .withColumn(\"_ingested_at\", F.current_timestamp())\n",
    "#                 )\n",
    "#                 for c in df.columns:\n",
    "#                     new_col = sanitize_name(c)\n",
    "#                     if new_col != c:\n",
    "#                         df = df.withColumnRenamed(c, new_col)\n",
    "\n",
    "#                 df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").save(output_path)\n",
    "#                 log_ingestion(table_name, file.name, file_mod_iso)\n",
    "\n",
    "#                 spark.sql(f\"\"\"\n",
    "#                 CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table_name}\n",
    "#                 USING DELTA\n",
    "#                 LOCATION '{output_path}'\n",
    "#                 \"\"\")\n",
    "\n",
    "#                 print(f\"Overwrote {table_name} with {file.name}\")\n",
    "\n",
    "#     print(\"Bronze ingestion run complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bde3e531-be4c-4a90-b443-1b8c89a80766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime, timezone\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "def ingest_latest_files(bronze_path: str, catalog: str, schema: str):\n",
    "    def is_file_already_ingested(table_name: str, file_name: str, file_mod_iso: str) -> bool:\n",
    "        if DeltaTable.isDeltaTable(spark, ingestion_log_path):\n",
    "            log_df = spark.read.format(\"delta\").load(ingestion_log_path)\n",
    "            return (\n",
    "                log_df.filter(\n",
    "                    (F.col(\"table_name\") == table_name) & \n",
    "                    (F.col(\"file_name\") == file_name) &\n",
    "                    (F.col(\"file_mod_ts\") == file_mod_iso)\n",
    "                )\n",
    "                .limit(1)\n",
    "                .count() > 0\n",
    "            )\n",
    "        return False\n",
    "\n",
    "    def log_ingestion(table_name: str, file_name: str, file_mod_iso: str):\n",
    "        row = [(table_name, file_name, file_mod_iso,\n",
    "                datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"))]\n",
    "\n",
    "        log_df = spark.createDataFrame(\n",
    "            row, schema=[\"table_name\", \"file_name\", \"file_mod_ts\", \"logged_at\"]\n",
    "        )\n",
    "\n",
    "        if DeltaTable.isDeltaTable(spark, ingestion_log_path):\n",
    "            log_df.write.format(\"delta\").mode(\"append\").save(ingestion_log_path)\n",
    "        else:\n",
    "            log_df.write.format(\"delta\").mode(\"overwrite\").save(ingestion_log_path)\n",
    "\n",
    "    #Listing subfolders in Bronze\n",
    "    subfolders = [f.path for f in dbutils.fs.ls(bronze_path) if f.isDir()]\n",
    "\n",
    "    for folder in subfolders:\n",
    "        table_name = os.path.basename(folder.rstrip(\"/\")).lower()\n",
    "        output_path = f\"{bronze_path}delta/{table_name}/\"\n",
    "\n",
    "       # finding csv files in each folder\n",
    "        files = [f for f in dbutils.fs.ls(folder) if f.name.endswith(\".csv\")]\n",
    "        if not files:\n",
    "            print(f\"No files found in {folder}\")\n",
    "            continue\n",
    "\n",
    "        #processing new files if found\n",
    "        for file in files:\n",
    "            file_mod_iso = datetime.utcfromtimestamp(\n",
    "                file.modificationTime / 1000.0\n",
    "            ).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            if is_file_already_ingested(table_name, file.name, file_mod_iso):\n",
    "                print(f\"Skipping {file.name} — already ingested.\")\n",
    "                continue\n",
    "\n",
    "            # Read CSV into DataFrame\n",
    "            df = (\n",
    "                spark.read.option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .csv(file.path)\n",
    "                .withColumn(\"_source_file\", F.lit(file.name))\n",
    "                .withColumn(\"_ingested_at\", F.to_timestamp(F.lit(file_mod_iso)))\n",
    "            )\n",
    "\n",
    "            # Standardize column names\n",
    "            for c in df.columns:\n",
    "                new_col = sanitize_name(c)\n",
    "                if new_col != c:\n",
    "                    df = df.withColumnRenamed(c, new_col)\n",
    "\n",
    "            # Append into Delta\n",
    "            df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(output_path)\n",
    "            log_ingestion(table_name, file.name, file_mod_iso)\n",
    "\n",
    "            # Register table in catalog\n",
    "            spark.sql(f\"\"\"\n",
    "              CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table_name}\n",
    "              USING DELTA\n",
    "              LOCATION '{output_path}'\n",
    "            \"\"\")\n",
    "\n",
    "            print(f\"Appended {file.name} into {table_name}\")\n",
    "\n",
    "    print(\"Bronze ingestion run complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7009a7-d1cf-4dae-a8a7-0ca789e25271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Running the ingestion funtion\n",
    "ingestion_log_path = f\"{bronze_path}ingestion_log/\"\n",
    "ingest_latest_files(bronze_path, catalog, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e2bae7-5f4b-4a09-9cec-bb65d1e7679e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.table(\"veersadatabricks.bronze.patientinfo\")\n",
    "# print(df.count())\n",
    "# df.filter(col(\"_source_file\") == \"PatientInfo - Copy.csv\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "24c273f1-07d2-4907-b06f-0b28b7d98098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime, timezone\n",
    "# from delta.tables import DeltaTable\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# def ingest_latest_files(bronze_path, catalog, schema):\n",
    "#     # Ensure schema is set\n",
    "#     if schema is None:\n",
    "#         raise ValueError(\"Schema is None. Make sure your environment variable is loaded correctly.\")\n",
    "\n",
    "#     # Config: where to keep the ingestion log (Delta)\n",
    "#     ingestion_log_path = f\"{bronze_path}ingestion_log/\"   # delta path for ingestion audit/log\n",
    "#     patient_table_name = \"patientinfo\"                    # sanitized name you use for patient table\n",
    "\n",
    "#     # helper to check if a file already logged for a table\n",
    "#     def is_file_already_ingested(table_name, file_name):\n",
    "#         if DeltaTable.isDeltaTable(spark, ingestion_log_path):\n",
    "#             log_df = spark.read.format(\"delta\").load(ingestion_log_path)\n",
    "#             return log_df.filter((col(\"table_name\") == table_name) & (col(\"file_name\") == file_name)).limit(1).count() > 0\n",
    "#         return False\n",
    "\n",
    "#     # helper to append a log row\n",
    "#     def log_ingestion(table_name, file_name, file_mod_iso):\n",
    "#         row = [(table_name, file_name, file_mod_iso, datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"))]\n",
    "#         log_df = spark.createDataFrame(row, schema=[\"table_name\",\"file_name\",\"file_mod_ts\",\"logged_at\"])\n",
    "#         if DeltaTable.isDeltaTable(spark, ingestion_log_path):\n",
    "#             log_df.write.format(\"delta\").mode(\"append\").save(ingestion_log_path)\n",
    "#         else:\n",
    "#             log_df.write.format(\"delta\").mode(\"overwrite\").save(ingestion_log_path)\n",
    "\n",
    "#     # List all subfolders in Bronze (same as your original)\n",
    "#     subfolders = [f.path for f in dbutils.fs.ls(bronze_path) if f.isDir() and not f.name.startswith(\"_\") and f.name != \"delta/\"]\n",
    "\n",
    "#     for folder in subfolders:\n",
    "#         table_name = sanitize_name(os.path.basename(folder.rstrip(\"/\")))\n",
    "\n",
    "#         # List files in subfolder\n",
    "#         files = [f for f in dbutils.fs.ls(folder) if f.name.endswith(\".csv\")]\n",
    "#         if not files:\n",
    "#             print(f\"No files found in {folder}\")\n",
    "#             continue\n",
    "\n",
    "#         # Pick file with latest modification time\n",
    "#         latest_file = sorted(files, key=lambda f: f.modificationTime)[-1]\n",
    "#         input_path = latest_file.path\n",
    "#         output_path = f\"{bronze_path}delta/{table_name}/\"\n",
    "\n",
    "#         print(f\"Processing file {latest_file.name} for table {schema}.{table_name}\")\n",
    "\n",
    "#         # Convert file mod time (ms since epoch) -> ISO timestamp string\n",
    "#         file_mod_iso = datetime.utcfromtimestamp(latest_file.modificationTime / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#         # Read CSV\n",
    "#         df = (spark.read\n",
    "#               .option(\"header\", \"true\")\n",
    "#               .option(\"inferSchema\", \"true\")\n",
    "#               .csv(input_path)\n",
    "#         )\n",
    "\n",
    "#         # Add deterministic metadata columns (crucial for idempotency / SCD2)\n",
    "#         df = df.withColumn(\"_source_file\", F.lit(latest_file.name))\\\n",
    "#                .withColumn(\"ingested_at\", F.to_timestamp(F.lit(file_mod_iso)))  # deterministic timestamp\n",
    "#         df =  df.withColumn(\"_source_file_mod_ts\", F.to_timestamp(F.lit(file_mod_iso)))\n",
    "#         # Sanitize column names (preserve newly added metadata columns)\n",
    "#         for c in df.columns:\n",
    "#             new_col = sanitize_name(c)\n",
    "#             if new_col != c:\n",
    "#                 df = df.withColumnRenamed(c, new_col)\n",
    "\n",
    "#         # --- Special behavior for patientinfo: idempotent append + ingestion log ---\n",
    "#         if table_name == patient_table_name:\n",
    "#             if is_file_already_ingested(table_name, latest_file.name):\n",
    "#                 print(f\"Skipping {latest_file.name} for {table_name} — already ingested.\")\n",
    "#                 continue\n",
    "\n",
    "#             # Append the new snapshot (will create table if not exists)\n",
    "#             df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(output_path)\n",
    "#             # record ingestion\n",
    "#             log_ingestion(table_name, latest_file.name, file_mod_iso)\n",
    "\n",
    "#             # Register table in catalog if desired (safe even if already exists)\n",
    "#             try:\n",
    "#                 spark.sql(f\"\"\"\n",
    "#                   CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table_name}\n",
    "#                   USING DELTA\n",
    "#                   LOCATION '{output_path}'\n",
    "#                 \"\"\")\n",
    "#             except Exception as e:\n",
    "#                 print(\"Could not register table in catalog (maybe catalog not configured):\", e)\n",
    "\n",
    "#             print(f\"Ingested {latest_file.name} -> {output_path} (append)\")\n",
    "\n",
    "#         else:\n",
    "#             # Default/legacy behavior: full overwrite (unchanged)\n",
    "#             df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(output_path)\n",
    "\n",
    "#             # Register as external table in catalog\n",
    "#             try:\n",
    "#                 spark.sql(f\"\"\"\n",
    "#                   CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table_name}\n",
    "#                   USING DELTA\n",
    "#                   LOCATION '{output_path}'\n",
    "#                 \"\"\")\n",
    "#             except Exception as e:\n",
    "#                 print(\"Could not register table in catalog (maybe catalog not configured):\", e)\n",
    "\n",
    "#             print(f\"Overwrote {output_path} with {latest_file.name}\")\n",
    "\n",
    "#     print(\"Ingestion run complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9e9c5acd-97b6-4a0f-8c2c-cf30285d38ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def ingest_csv_folder(bronze_path, catalog, schema):\n",
    "#     # List all files in Bronze folder\n",
    "#     files = dbutils.fs.ls(bronze_path)\n",
    "    \n",
    "#     for file in files:\n",
    "#         if file.name.endswith(\".csv\"):  # process only CSV files\n",
    "#             table_name = os.path.splitext(file.name)[0]  # filename without .csv\n",
    "#             input_path = bronze_path + file.name\n",
    "#             output_path = bronze_path + f\"delta/{table_name}/\"\n",
    "\n",
    "#             print(f\"Processing {file.name} → table {schema}.{table_name}\")\n",
    "\n",
    "#             # Read CSV\n",
    "#             df = (spark.read\n",
    "#                   .option(\"header\", \"true\")\n",
    "#                   .option(\"inferSchema\", \"true\")\n",
    "#                   .csv(input_path)\n",
    "#                   .withColumn(\"_ingestion_date\", current_timestamp())\n",
    "#                   .withColumn(\"_source_file\", input_file_name()))\n",
    "\n",
    "#             # Write to Delta\n",
    "#             df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "\n",
    "#             # Register as external Delta table\n",
    "#             spark.sql(f\"\"\"\n",
    "#               CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table_name}\n",
    "#               USING DELTA\n",
    "#               LOCATION '{output_path}'\n",
    "#             \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a85aa3ca-ae0e-4249-974b-5b72932de325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def ingest_latest_files(bronze_path, catalog, schema):\n",
    "#     # Ensure schema is set\n",
    "#     if schema is None:\n",
    "#         raise ValueError(\"Schema is None. Make sure your environment variable is loaded correctly.\")\n",
    "\n",
    "#     # List all subfolders in Bronze\n",
    "#     subfolders = [f.path for f in dbutils.fs.ls(bronze_path) if f.isDir() and not f.name.startswith(\"_\") and f.name != \"delta/\"]\n",
    "\n",
    "#     for folder in subfolders:\n",
    "#         table_name = sanitize_name(os.path.basename(folder.rstrip(\"/\")))\n",
    "\n",
    "#         # List files in subfolder\n",
    "#         files = [f for f in dbutils.fs.ls(folder) if f.name.endswith(\".csv\")]\n",
    "#         if not files:\n",
    "#             print(f\"No files found in {folder}\")\n",
    "#             continue\n",
    "\n",
    "#         # Pick file with latest modification time\n",
    "#         latest_file = sorted(files, key=lambda f: f.modificationTime)[-1]\n",
    "#         input_path = latest_file.path\n",
    "#         output_path = f\"{bronze_path}delta/{table_name}/\"\n",
    "\n",
    "#         print(f\"Loading {latest_file.name} into table {schema}.{table_name}\")\n",
    "\n",
    "#         # Read CSV\n",
    "#         df = (spark.read\n",
    "#               .option(\"header\", \"true\")\n",
    "#               .option(\"inferSchema\", \"true\")\n",
    "#               .csv(input_path)\n",
    "#               .withColumn(\"ingested_at\", F.current_timestamp())\n",
    "#               #.withColumn(\"_source_file\", F.col(\"_metadata.file_path\"))\n",
    "#         )\n",
    "\n",
    "#         # Sanitize column names\n",
    "#         for col in df.columns:\n",
    "#             new_col = sanitize_name(col)\n",
    "#             if new_col != col:\n",
    "#                 df = df.withColumnRenamed(col, new_col)\n",
    "\n",
    "#         # Write to Delta\n",
    "#         df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(output_path)\n",
    "\n",
    "#         # Register as external table in catalog\n",
    "#         spark.sql(f\"\"\"\n",
    "#           CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{table_name}\n",
    "#           USING DELTA\n",
    "#           LOCATION '{output_path}'\n",
    "#         \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3ed89e0d-7bd2-46c8-9ac3-6c68b741d150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Databricks notebook source\n",
    "# # MAGIC %pip install python-dotenv\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# import os\n",
    "# import requests\n",
    "# from dotenv import load_dotenv\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# # Env variables\n",
    "# DATABRICKS_INSTANCE = os.getenv(\"DATABRICKS_INSTANCE\")\n",
    "# CONFIG_JOB_ID = os.getenv(\"CONFIG_JOB_ID\")\n",
    "# TOKEN = os.getenv(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "# # Your schema namespace (change if needed)\n",
    "# SCHEMA_NAME = \"agent_productivity.config\"\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# # Run Databricks Job\n",
    "# def run_job(job_id: str):\n",
    "#     url = f\"{DATABRICKS_INSTANCE}/api/2.1/jobs/run-now\"\n",
    "#     headers = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "#     payload = {\"job_id\": job_id}\n",
    "\n",
    "#     response = requests.post(url, headers=headers, json=payload)\n",
    "#     response.raise_for_status()\n",
    "#     return response.json()\n",
    "\n",
    "# # Guard clause → ensure schema exists\n",
    "# if not spark.catalog.databaseExists(SCHEMA_NAME):\n",
    "#     print(f\"Schema {SCHEMA_NAME} missing → running job {CONFIG_JOB_ID}\")\n",
    "#     run_output = run_job(CONFIG_JOB_ID)\n",
    "#     print(\"Job triggered:\", run_output)\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# # Retry wrapper for notebook runs\n",
    "# def run_with_retry(notebook, timeout, max_retries=3):\n",
    "#     num_retries = 0\n",
    "#     while True:\n",
    "#         try:\n",
    "#             return dbutils.notebook.run(notebook, timeout)\n",
    "#         except Exception as e:\n",
    "#             if num_retries >= max_retries:\n",
    "#                 raise e\n",
    "#             else:\n",
    "#                 print(f\"Retrying ({num_retries+1}/{max_retries}) due to error: {e}\")\n",
    "#                 num_retries += 1\n",
    "\n",
    "# # Example usage\n",
    "# update_config = run_with_retry(\n",
    "#     \"/Users/your_email/ingestion/update_config\", \n",
    "#     0, \n",
    "#     max_retries=3\n",
    "# )\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# # Utility: clean header rows (avoid duplicate headers in CSVs)\n",
    "# def clean_header_rows(df, schema):\n",
    "#     first_col = schema.fields[0].name\n",
    "#     return df.filter(F.col(first_col) != first_col)\n",
    "\n",
    "# # Utility: add metadata column\n",
    "# def with_metadata(df):\n",
    "#     return df.withColumn(\"ingested_at\", F.current_timestamp())\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# # Example ingestion function\n",
    "# def reader(src):\n",
    "#     # Infer schema once from latest file\n",
    "#     schema = (spark.read\n",
    "#               .format(\"csv\")\n",
    "#               .option(\"header\", \"true\")\n",
    "#               .load(src[\"latest_file_path\"]).schema)\n",
    "\n",
    "#     # Read batch (not streaming for now)\n",
    "#     df = (spark.read\n",
    "#           .format(\"csv\")\n",
    "#           .option(\"header\", \"true\")\n",
    "#           .schema(schema)\n",
    "#           .load(src[\"folder_path\"]))\n",
    "\n",
    "#     df = clean_header_rows(df, schema)\n",
    "#     df = with_metadata(df)\n",
    "#     return df\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# # Load config\n",
    "# CONFIG_PATH = \"agent_productivity.config.ingestion_config\"\n",
    "# sources = spark.table(CONFIG_PATH)\n",
    "\n",
    "# # Process sources\n",
    "# for src in sources.collect():\n",
    "#     df = reader(src)\n",
    "#     target = f\"agent_productivity.ingestion.{src['table_name']}\"\n",
    "#     spark.sql(\"CREATE SCHEMA IF NOT EXISTS agent_productivity.ingestion\")\n",
    "\n",
    "#     # Write batch\n",
    "#     df.write.mode(\"append\").saveAsTable(target)\n",
    "#     print(f\"Ingested {src['table_name']} into Bronze\")\n",
    "\n",
    "# # Update config table with ingestion timestamp\n",
    "# sources.withColumn(\"last_ingestion_time\", F.current_timestamp()) \\\n",
    "#        .write.mode(\"overwrite\").saveAsTable(CONFIG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e16d039c-d131-4d2e-88ca-e39cec0ba87b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Ingestion log path\n",
    "# ingestion_log_path = f\"{bronze_path}ingestion_log/\"\n",
    "\n",
    "#     # --- Clear log at the beginning ---\n",
    "# if DeltaTable.isDeltaTable(spark, ingestion_log_path):\n",
    "#     spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.bronze_ingestion_log\")\n",
    "#     dbutils.fs.rm(ingestion_log_path, recurse=True)\n",
    "# print(\"✅ Cleared ingestion log. Fresh run starting.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4879340126524932,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
